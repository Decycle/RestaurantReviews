{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the necessary packages\n",
    "Recommend using Linux or WSL for Windows.\n",
    "A good Nvidia GPU (rtx-30xx/40xx) is required for reasonable speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following install the required package for cuda 12.1 in Linux, for the newer RTX 30xx GPUs or higher "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade pip\n",
    "!python -m pip install --upgrade --force-reinstall --no-cache-dir torch==2.2.0 triton \\\n",
    "  --index-url https://download.pytorch.org/whl/cu121\n",
    "!python -m pip install \"unsloth[cu121-ampere-torch220] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If installed without failure, starts by downloading the model we are gonna use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
    "    dtype=None,        # None for auto detect\n",
    "    load_in_4bit=True,\n",
    ") # about 6GB in size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Rate the following restaurant in the category of ??? from 1 to 5 where 1 means ##### and 5 means ######.\"},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "rating_ids = []\n",
    "for score in [1,2,3,4,5]:\n",
    "    rating_ids.append(tokenizer.encode(score))\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids=input_ids)[-1]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
