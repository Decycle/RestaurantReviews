{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Decoder Inference of the Restaurant Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the necessary packages\n",
    "Recommend using Linux or WSL for Windows.\n",
    "A good Nvidia GPU (rtx-30xx/40xx) is required for reasonable speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following install the required package for cuda 12.1 in Linux, for the newer RTX 30xx GPUs or higher "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install --upgrade pip\n",
    "# !python -m pip install --upgrade --force-reinstall --no-cache-dir torch==2.2.0 triton \\\n",
    "#   --index-url https://download.pytorch.org/whl/cu121\n",
    "# !python -m pip install \"unsloth[cu121-ampere-torch220] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from unsloth import FastLanguageModel\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import numpy as np\n",
    "from tqdm import trange, tqdm # progress bar\n",
    "from matplotlib import pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If installed without failure, starts by downloading the model we are gonna use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
    "    dtype=None,        # None for auto detect\n",
    "    load_in_4bit=True,\n",
    ") # about 6GB in size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire Dataset\n",
    "\n",
    "We will be using the yelp dataset for experimenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/la_mini_df.csv',sep='|', encoding='utf-8', escapechar='\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = df[df['text'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Dataset for Testing\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(df_reviews)\n",
    "dataset.set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "num_batch = len(dataset) // BATCH_SIZE\n",
    "DATASET_SIZE = num_batch * BATCH_SIZE\n",
    "\n",
    "dataset = dataset.select(range(DATASET_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesssing\n",
    "\n",
    "Now let's set up the function to format the prompt according to LLama 3's specication, as well as tokenizing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"FOOD\", \"LOCATION\", \"ATOMSPHERE\", \"SERVICE\", \"PRICE\", \"MENU\", \"SPEED\"]\n",
    "\n",
    "def format_prompt(category, review):\n",
    "    # review = 'The waitress is nice and the food is the best thing i have ever had, it is so good! However, the atomsphere sucks and i hate how loud it is.'\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "        {\"role\": \"user\", \"content\": f'Rate the following restaurant review in the category of {category} from 1 to 5 where 1 means the worst possible and 5 means the best in their life. Only rate how good the {category} is. Do not pay attention to other factors. If the category {category} is not mentioned in the review, output \"NOT MENTIONED\" instead. Review: \"{review}\" '},\n",
    "        {\"role\": \"assistant\", \"content\": f\"The rating of {category} is: \"}\n",
    "    ]\n",
    "    # The apply_chat_template auto inserts a [eot_id] token at the end, which we will discard.\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "    )[:-10]\n",
    "    return prompt\n",
    "\n",
    "def preprocess(batch):\n",
    "    reviews = batch['text']\n",
    "    results = {}\n",
    "    for category in categories:\n",
    "        prompts = []\n",
    "        for review in reviews:\n",
    "            prompts.append(format_prompt(category, review))\n",
    "\n",
    "        results[category] = prompts\n",
    "    return results\n",
    "\n",
    "dataset = dataset.map(preprocess, batched=True, batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's checkout the length distrbution of the review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_prmopt_length = preprocess({\"text\": [\"\"]})\n",
    "empty_prmopt_length = tokenizer(empty_prmopt_length['ATOMSPHERE'], return_tensors='pt')['input_ids'].shape[1]\n",
    "empty_prmopt_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(\n",
    "    dataset['ATOMSPHERE'][:10000],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "input_ids = tokens.input_ids\n",
    "attention_mask = tokens.attention_mask\n",
    "\n",
    "token_lengths = torch.sum(attention_mask, dim=-1)\n",
    "\n",
    "# Plot the distribution of token lengths\n",
    "plt.hist(token_lengths, bins=120)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_index = token_lengths.topk(1, largest=False).indices.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_lengths = token_lengths.to(torch.float32)\n",
    "\n",
    "print(f\"Mean: {torch.mean(token_lengths).item()}\")\n",
    "print(f\"Std: {torch.std(token_lengths).item()}\")\n",
    "print(f\"95th percentile: {torch.quantile(token_lengths, 0.95).item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the histogram, we can see that 95% review has length of less than 284. We will use that as the max length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 284"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's batch up the data and send it to GPU before the actual inteference. This should hopefully speed things up because there is now less cpu to gpu communication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_input_ids = {}\n",
    "batched_attention_masks = {}\n",
    "\n",
    "for category in tqdm(categories):\n",
    "    tokens = tokenizer(\n",
    "        dataset[category],\n",
    "        return_tensors=\"pt\",\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH\n",
    "    )\n",
    "\n",
    "    input_ids = tokens.input_ids.to(model.device)\n",
    "    attention_mask = tokens.attention_mask.to(model.device)\n",
    "\n",
    "    input_ids = input_ids.reshape(-1, BATCH_SIZE, MAX_LENGTH)\n",
    "    attention_mask = attention_mask.reshape(-1, BATCH_SIZE, MAX_LENGTH)\n",
    "\n",
    "    batched_input_ids[category] = input_ids\n",
    "    batched_attention_masks[category] = attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "We will use the probability output of the model for the last token.\n",
    "\n",
    "We will record the logits for each of the rating number, as well as the probability of the model saying \"NOT\".\n",
    "\n",
    "Let the logits for rating number $i$ be $l_i$, the score can be cacluated as\n",
    "\n",
    "$$s = \\frac{\\sum_{i=1}^5 i \\exp(l_i)}{\\sum_{i=1}^5 \\exp(l_i)}$$\n",
    "for that category.\n",
    "\n",
    "If the probablity of the model saying \"NOT\" is very high, that means the model thinks the category is not mentioned in the review. In that case we should discard the rating.\n",
    "\n",
    "In particular, denote the logits for predicting NOT as $l_n$, we can extract a \"usefulness\" parameter $u$\n",
    "\n",
    "$$u = \\frac{\\sum_{i=1}^5 \\exp(l_i)}{exp(l_n) + \\sum_{i=1}^5 \\exp(l_i)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "    df[f'{category}_score'] = pd.Series(dtype='float')\n",
    "    df[f'{category}_usefulness'] = pd.Series(dtype='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3754/3754 [1:25:12<00:00,  1.36s/it]\n",
      "100%|██████████| 3754/3754 [1:25:12<00:00,  1.36s/it]\n",
      "100%|██████████| 3754/3754 [1:38:21<00:00,  1.57s/it]  \n",
      " 81%|████████  | 3041/3754 [1:09:04<16:16,  1.37s/it]"
     ]
    }
   ],
   "source": [
    "# llama 3 stores digit id as 15 + that digit\n",
    "numbers = torch.tensor([1,2,3,4,5]).to(model.device)\n",
    "indices = numbers + 15\n",
    "\n",
    "# the id for the word \"NOT\" is 14394\n",
    "# If the model has a high probability of predicting this word, it means the review score is not going to be useful\n",
    "not_id = 14394\n",
    "\n",
    "review_scores = torch.zeros(len(categories), len(dataset))\n",
    "review_usefulness= torch.zeros(len(categories), len(dataset))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, category in enumerate(categories):\n",
    "        for j in trange(num_batch):\n",
    "            input_ids = batched_input_ids[category][j]\n",
    "            attention_mask = batched_attention_masks[category][j]\n",
    "            # get the logits for the 5 number we are interested in\n",
    "            logits = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                output_hidden_states=False,\n",
    "                use_cache=False\n",
    "                ).logits[:, -1]\n",
    "\n",
    "            number_logits = logits[:, indices]\n",
    "            exp_logits = torch.exp(number_logits)\n",
    "            exp_logits_sum = torch.sum(exp_logits, dim=1)\n",
    "\n",
    "            not_logits = logits[:, not_id]\n",
    "            exp_not_logits = torch.exp(not_logits)\n",
    "\n",
    "            usefulness = exp_not_logits / (exp_not_logits + exp_logits_sum)\n",
    "            review_usefulness[i, j*BATCH_SIZE:(j+1)*BATCH_SIZE] = usefulness\n",
    "\n",
    "            scores = torch.sum(exp_logits * numbers, dim=1) / exp_logits_sum\n",
    "            review_scores[i, j*BATCH_SIZE:(j+1)*BATCH_SIZE] = scores\n",
    "\n",
    "            df_ids = dataset['__index_level_0__'][j*BATCH_SIZE:(j+1)*BATCH_SIZE]\n",
    "            df.loc[df_ids, f'{category}_score'] = scores.to(torch.float16).cpu().numpy()\n",
    "            df.loc[df_ids, f'{category}_usefulness'] = usefulness.to(torch.float16).cpu().numpy()\n",
    "        df.to_csv(f'dataset/la_mini_{category}.csv', sep='|', encoding='utf-8', escapechar='\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['__index_level_0__']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['FOOD'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    input_ids = tokenizer(\n",
    "        dataset['FOOD'][0],\n",
    "        return_tensors=\"pt\"\n",
    "    )['input_ids'].to(model.device)\n",
    "    logits = model(\n",
    "                input_ids=input_ids,\n",
    "                output_hidden_states=False,\n",
    "                use_cache=False\n",
    "                ).logits[:, -1]\n",
    "    number_logits = logits[:, indices]\n",
    "    exp_logits = torch.exp(number_logits)\n",
    "    exp_logits_sum = torch.sum(exp_logits, dim=1)\n",
    "\n",
    "    not_logits = logits[:, not_id]\n",
    "    exp_not_logits = torch.exp(not_logits)\n",
    "\n",
    "    usefulness = exp_not_logits / (exp_not_logits + exp_logits_sum)\n",
    "    scores = torch.sum(exp_logits * numbers, dim=1) / exp_logits_sum\n",
    "\n",
    "scores, usefulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze\n",
    "\n",
    "Let's plot the review usefulness to see just how much review can we consider as useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usefulness = review_usefulness[:, ].flatten().cpu().numpy()\n",
    "\n",
    "plt.hist(usefulness, bins=20)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly the model thinks that 2/3 of the reviews is not helpful. This is to be expected because a normal review won't cover all the categories.\n",
    "\n",
    "Let's choose a usefulness threshold of 0.5 for the mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_mask = review_usefulness > 0.7\n",
    "\n",
    "print(f\"Useful Review: {torch.sum(review_mask).item() / review_mask.numel() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the review_mask and review_scores just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musefulness.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m, review_usefulness\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m      2\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscores.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m, review_scores\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()) \u001b[38;5;66;03m# Good thing that we didn't shuffle the dataset!\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "np.save(\"usefulness.npy\", review_usefulness.cpu().numpy())\n",
    "np.save(\"scores.npy\", review_scores.cpu().numpy()) # Good thing that we didn't shuffle the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_mask = review_usefulness > 0.5\n",
    "\n",
    "# let's see how much responses we are ignoring\n",
    "for i, category in enumerate(categories):\n",
    "    print(f\"{category}: {num_batch*BATCH_SIZE - torch.sum(review_mask[i]).item()} out of {num_batch*BATCH_SIZE} reviews are ignored\")\n",
    "\n",
    "# let's also see how much useful category does each review have\n",
    "useful_reviews = torch.sum(review_mask[:, :num_batch*BATCH_SIZE], dim=0)\n",
    "plt.hist(useful_reviews, bins=len(categories), width=0.7)\n",
    "plt.title(\"Number of Useful Categories per Review\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's insepct somes reviews that are relevant to the location. Just to make sure they are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_id = categories.index(\"LOCATION\")\n",
    "index = torch.nonzero(review_mask[location_id] == True)\n",
    "\n",
    "# let's see some of the reviews that are useful for location\n",
    "for i in range(10):\n",
    "    print(dataset['text'][index[i].item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of these reviews have mentioned something about the location, which means the model behaves as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_scores = dataset['label']\n",
    "\n",
    "actual_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_scores.shape, review_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each category, let's calculate the spearmanr and pearsonr corrleation to see how relevant they are. We will only consider the reviews that is useful to that category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spearsman and pearson correlation\n",
    "spearmans = []\n",
    "pearsons = []\n",
    "for i, category in enumerate(categories):\n",
    "    mask = review_mask[i]\n",
    "    predicated_scores = review_scores[i][mask].cpu()\n",
    "    real_scores = actual_scores[mask]\n",
    "    spearman = spearmanr(predicated_scores, real_scores.cpu())\n",
    "    pearson = pearsonr(predicated_scores, real_scores.cpu())\n",
    "    spearmans.append(spearman)\n",
    "    pearsons.append(pearson)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.bar(categories, [pearson[0] for pearson in pearsons], label='Pearson')\n",
    "plt.bar(categories, [spearman.correlation for spearman in spearmans], label='Spearman')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the box plot of ratings for each category\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "for i, category in enumerate(categories):\n",
    "    mask = review_mask[i]\n",
    "    predicated_scores = review_scores[i][mask].cpu()\n",
    "    real_scores = actual_scores[mask]\n",
    "\n",
    "    plt.boxplot(predicated_scores, positions=[i], showfliers=False)\n",
    "\n",
    "plt.xticks(range(len(categories)), categories)\n",
    "plt.title(\"Predicted Scores\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also export the reviews to a csv file for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usefulness = np.load(\"usefulness.npy\")\n",
    "scores = np.load(\"scores.npy\")\n",
    "\n",
    "df = dataset.to_pandas()\n",
    "for i, category in enumerate(categories):\n",
    "    df[f\"{category}_usefulness\"] = usefulness[i]\n",
    "    df[f\"{category}_score\"] = scores[i]\n",
    "\n",
    "# save the dataframe\n",
    "df.to_csv(\"yelp_restaurant_review_labelled.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
