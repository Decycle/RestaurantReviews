{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the necessary packages\n",
    "Recommend using Linux or WSL for Windows.\n",
    "A good Nvidia GPU (rtx-30xx/40xx) is required for reasonable speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following install the required package for cuda 12.1 in Linux, for the newer RTX 30xx GPUs or higher "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade pip\n",
    "!python -m pip install --upgrade --force-reinstall --no-cache-dir torch==2.2.0 triton \\\n",
    "  --index-url https://download.pytorch.org/whl/cu121\n",
    "!python -m pip install \"unsloth[cu121-ampere-torch220] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If installed without failure, starts by downloading the model we are gonna use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/llama-3-8b-bnb-4bit\",\n",
    "    dtype=None,        # None for auto detect\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
